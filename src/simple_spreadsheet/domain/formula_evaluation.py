from typing import Union
from domain.coordinates import Coordinates

OPERATORS_AND_DELIMITERS = {'+', '-', '*', '/', '(', ')', ':', ';'}
FORMULAS = {'SUMA', 'PROMEDIO', 'MAX', 'MIN'}
DECIMAL_SEPARATOR = '.'


class Tokenizer:
    def __init__(self) -> None:
        self.__tokens: list[Union[str, Coordinates]] = []

    def _normalize_expression(self, expression: str) -> str:
        """Removes unnecessary spaces from the expression."""
        return expression.replace(' ', '')

    def _is_valid_number_character(self, char: str) -> bool:
        """Checks if a character is a digit or the decimal separator."""
        return char.isdigit() or char == DECIMAL_SEPARATOR

    def _extract_number(self, expression: str, start_index: int) -> tuple[float, int]:
        """Extracts a numeric token from the expression."""
        num = ''
        n = len(expression)
        i = start_index
        while i < n and self._is_valid_number_character(expression[i]):
            if expression[i] == DECIMAL_SEPARATOR and DECIMAL_SEPARATOR in num:
                raise ValueError(f'Invalid number format at position {i}')
            num += expression[i]
            i += 1
        return float(num), i

    def _extract_cell_or_formula(self, expression: str, start_index: int) -> tuple[Union[str, float, Coordinates], int]:
        """Extracts a cell reference or formula from the expression."""
        var = ''
        n = len(expression)
        i = start_index
        while i < n and (expression[i].isalpha() or expression[i].isdigit()):
            var += expression[i]
            i += 1

        if var.isalpha():  # All alphabetic characters
            upper_var = var.upper()
            if upper_var in FORMULAS:
                return upper_var, i
            raise ValueError(
                f'Invalid formula "{var}" at position {start_index}')
        return Coordinates.from_id(var), i

    def tokenize(self, expression: str) -> list[Union[str, Coordinates]]:
        """Tokenizes the given mathematical expression."""
        self.__tokens = []
        expression = self._normalize_expression(expression)
        n = len(expression)
        i = 0

        while i < n:
            char = expression[i]
            if self._is_valid_number_character(char):  # Handle numbers
                num, i = self._extract_number(expression, i)
                self.__tokens.append(num)
            elif char.isalpha():  # Handle cells and formulas
                var, i = self._extract_cell_or_formula(expression, i)
                self.__tokens.append(var)
            elif char in OPERATORS_AND_DELIMITERS:  # Handle operators and delimiters
                self.__tokens.append(char)
                i += 1
            else:  # Invalid character
                raise ValueError(f'Invalid character "{char}" at position {i}')

        return self.__tokens

    def get_tokens(self) -> list[Union[str, float, Coordinates]]:
        """Returns the tokens generated by the last call to tokenize."""
        return self.__tokens
